{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colaboratoryで実行する場合\n",
    "以下を実行して、外部ファイルをダウンロードしてください。   \n",
    "**このセルはColaboratoryを起動するたびに必要となります**   \n",
    "**「ランタイム＞ランタイムのタイプを変更」で「ハードウェアアクセラレータ」をGPUにしてから実行することをお勧めします。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "### Colaboratoryのみ以下を実行 ###\n",
    "##################################\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !wget -P ./img http://www.hal.t.u-tokyo.ac.jp/~yamakata/lecture/mediaproc/mediaproc6/mediaproc6-openpose.zip\n",
    "    !unzip img/mediaproc6-openpose.zip -d img/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenPoseで体の姿勢を推定してみよう！\n",
    "\n",
    "OpenPoseとは、コンピュータビジョンの国際学会CVPR2017でカーネギーメロン大学の研究チームが発表した、Deep Learningによる姿勢推定の手法です。   \n",
    "\n",
    "```\"Nose\", \"Neck\", \"RShoulder\", \"RElbow\", \"RWrist\", \"LShoulder\", \"LElbow\", \"LWrist\", \"RHip\", \"RKnee\", \"RAnkle\", \"LHip\", \"LKnee\", \"LAnkle\", \"REye\", \"LEye\", \"REar\", \"LEar\", \"Background\"```\n",
    "\n",
    "の計19個の関節や部位を見つけてくれます。   \n",
    "従来手法に比べ、左手と右手を区別することができるなど、非常に精度が高いことから、人間行動分析などでも使われています。\n",
    "\n",
    "- 論文：Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh, \"Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields\", CVPR2017. view on https://arxiv.org/abs/1611.08050\n",
    "- ソースコード：[GitHub](https://github.com/CMU-Perceptual-Computing-Lab/openpose)\n",
    "\n",
    "**この演習はマシンパワーを必要とするので、低スペックのPCだとフリーズする可能性があります。   \n",
    "PCのスペックに不安のある方は、Google Colaboratoryを利用してください。**\n",
    "\n",
    "Deep Learningによる画像認識にはいろいろなプラットフォームと実装がありますが、\n",
    "ここではopencv3.3以上に組み込まれたDeep LearningのライブラリCaffeを使いたいと思います。   \n",
    "物体検出プログラムを実行するには、学習済みのモデルとプロトテキストが必要です。\n",
    "\n",
    "- prototxt: ニューラルネットの構造を記述するためのCaffe独自のテキスト形式\n",
    "- caffeemodel: Caffe形式の学習済みモデル\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. モデルのダウンロード\n",
    "\n",
    "学習済みのモデルをダウンロードします。   \n",
    "\n",
    "**200MBのファイルです！非常に時間がかかります！従量課金されるネットワーク下では実行しないでください。**\n",
    "\n",
    "以下を**1回のみ**実行してください。  \n",
    "ダウンロードされたモデルは、このnotebookが置かれているフォルダ内にある、OpenPoseという名前のフォルダの下に入ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# この作業は1回のみで結構です！1回だけ、以下の「'''」を消してから実行してください。\n",
    "\n",
    "'''\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "if not os.path.exists('OpenPose'):\n",
    "    os.mkdir('OpenPose')\n",
    "\n",
    "# モデルのダウンロード元\n",
    "caffemodel_link = 'http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/coco/pose_iter_440000.caffemodel'\n",
    "prototxt_link = 'https://raw.githubusercontent.com/opencv/opencv_extra/master/testdata/dnn/openpose_pose_coco.prototxt'\n",
    "\n",
    "# モデルのダウンロード先\n",
    "caffemodel_save = 'OpenPose/pose_iter_440000.caffemodel'\n",
    "prototxt_save = 'OpenPose/openpose_pose_coco.prototxt'\n",
    "\n",
    "# ダウンロードの実行\n",
    "urllib.request.urlretrieve(caffemodel_link, caffemodel_save)\n",
    "urllib.request.urlretrieve(prototxt_link, prototxt_save)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 実行プログラム\n",
    "\n",
    "引数として与えられたprototxtとmodelを使い、入力画像imageについて物体検出する関数`OpenPose`を以下のように定義しま\n",
    "す。   \n",
    "なお、今回のやり方だと画像中に1名しか映っていない場合しか対応できませんが、OpenPoseの手法としては複数名移っていてもそれぞれ正しく姿勢を推定することが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 抽出対象の19点\n",
    "BODY_PARTS = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "               \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "               \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
    "               \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18 }\n",
    "BODY_PARTS_LIST = [\"Nose\", \"Neck\", \"RShoulder\", \"RElbow\", \"RWrist\",\n",
    "               \"LShoulder\", \"LElbow\", \"LWrist\", \"RHip\", \"RKnee\",\n",
    "               \"RAnkle\", \"LHip\", \"LKnee\", \"LAnkle\", \"REye\",\n",
    "               \"LEye\", \"REar\", \"LEar\", \"Background\"]\n",
    "\n",
    "# 繋がっている点の組み合わせ\n",
    "POSE_PAIRS = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "               [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
    "               [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n",
    "               [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],\n",
    "               [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"] ]\n",
    "\n",
    "label_colors = np.random.uniform(0, 255, size=(len(BODY_PARTS), 3)) # それぞれの部位に適当な色を割り当てる\n",
    "\n",
    "def OpenPose(image, prototxt, model, confidence):\n",
    "    inWidth = 368\n",
    "    inHeight = 368\n",
    "    inScaleFactor = 0.003922\n",
    "\n",
    "    # 画像の読み込み\n",
    "    image = cv2.cvtColor(cv2.imread(image), cv2.COLOR_BGR2RGB)\n",
    "    (h, w) = image.shape[:2] # 画像サイズの取得\n",
    "\n",
    "    # ネットワークの準備\n",
    "    dnn = cv2.dnn.readNetFromCaffe(prototxt, model)\n",
    "\n",
    "    # 画像を[inWidth x inHeight]にリサイズし、正規化してblob形式に変換\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(image, (inWidth, inHeight)), \n",
    "                                 inScaleFactor, \n",
    "                                 (inWidth, inHeight),\n",
    "                                 (0, 0, 0), \n",
    "                                 swapRB=False, \n",
    "                                 crop=False)\n",
    "    dnn.setInput(blob)\n",
    "\n",
    "    # モデルの適応\n",
    "    out = dnn.forward()\n",
    "\n",
    "    points = []\n",
    "    # 各部位ごとにその座標を特定していく\n",
    "    for i in range(len(BODY_PARTS)):\n",
    "        # ヒートマップを求める\n",
    "        heatMap = out[0, i, :, :]\n",
    "        \n",
    "        # 最も確率の高い1点を求める\n",
    "        _, conf, _, point = cv2.minMaxLoc(heatMap)\n",
    "        x = (w* point[0]) / out.shape[3]\n",
    "        y = (h * point[1]) / out.shape[2]\n",
    "        \n",
    "        # その部位において、もっとも確率が高かった位置の座標を記録\n",
    "        points.append((int(x), int(y)) if conf > confidence else None)\n",
    "\n",
    "    # 連結している部位のペアごとに線を引く\n",
    "    for pair in POSE_PAIRS:\n",
    "        # 連結元と連結先の部位の名前を取得\n",
    "        partFrom = pair[0]\n",
    "        partTo = pair[1]\n",
    "\n",
    "        # 連結元と連結先の部位のIDを取得\n",
    "        idFrom = BODY_PARTS[partFrom]\n",
    "        idTo = BODY_PARTS[partTo]\n",
    "\n",
    "        # 連結元と連結先の両方の点が見つかっていたら、その間に線を引く\n",
    "        if points[idFrom] and points[idTo]:\n",
    "            cv2.line(image, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "\n",
    "    # 各部位に対して点と名前を描画\n",
    "    for i in range(len(BODY_PARTS)):\n",
    "        cv2.ellipse(image, points[i], (6, 6), 0, 0, 360, label_colors[i], cv2.FILLED)\n",
    "        cv2.putText(image, BODY_PARTS_LIST[i], points[i], cv2.FONT_HERSHEY_SIMPLEX, 1, label_colors[i], 2)\n",
    "\n",
    "    # 画像の描画\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(image)\n",
    "    cv2.imwrite('img/OpenPose.jpg', cv2.cvtColor(image, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 画像中に写っている1名の姿勢推定\n",
    "\n",
    "入力画像をいろいろと変えて結果の変わり方を見てみましょう。   \n",
    "また、信頼度も変えてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検出した領域の信頼度に対する閾値（この値以上の信頼度の領域はOKとする）\n",
    "confidence = 0.1\n",
    "\n",
    "#image = 'img/action-active-activity-2091651.jpg' # https://www.pexels.comより取得\n",
    "#image = 'img/action-energy-active-adult-1921764.jpg' # https://www.pexels.comより取得\n",
    "#image = 'img/action-adult-athlete-936094.jpg' # https://www.pexels.comより取得\n",
    "image = 'img/action-adult-balance-1701194.jpg' # https://www.pexels.comより取得\n",
    "\n",
    "prototxt='OpenPose/openpose_pose_coco.prototxt'\n",
    "model=\"OpenPose/pose_iter_440000.caffemodel\"\n",
    "\n",
    "OpenPose(image, prototxt, model, confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "なお、OpenPoseは画像中に複数の人が写っている場合も、そのそれぞれの人の姿勢を推定することが可能な手法ですが、それを行うためにはもうひと手間必要になるので、ここでは1名のみが写っていると想定しています。   \n",
    "このプログラムでは複数名が映っている画像を入力すると、正しく姿勢が推定できません。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
