{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colaboratoryで実行する場合\n",
    "以下を実行して、外部ファイルをダウンロードしてください。   \n",
    "**このセルはColaboratoryを起動するたびに必要となります**   \n",
    "**<font color='red'>和文フォントをインストールしています。以下のセルを実行後、ランタイムをリスタートしてください。</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "### Colaboratoryのみ以下を実行 ###\n",
    "##################################\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !wget -P ./text http://www.hal.t.u-tokyo.ac.jp/~yamakata/lecture/mediaproc/mediaproc4/mediaproc4-2.zip\n",
    "    !unzip text/mediaproc4-2.zip -d text/\n",
    "    # 以下は日本語フォントをインストールするコマンドです\n",
    "    !apt-get -y install fonts-ipafont-gothic\n",
    "    !rm /root/.cache/matplotlib/*.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トピック分析2: tf-idfによる重要語抽出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 様々なトピックの記事の読み込み\n",
    "\n",
    "`text/wikipedia_wakati.json`には、[Wikipediaからダウンロード](https://ja.wikipedia.org/wiki/Wikipedia:%E3%83%87%E3%83%BC%E3%82%BF%E3%83%99%E3%83%BC%E3%82%B9%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89)してきたデータから、\n",
    "以下に挙げる６種類のカテゴリについて、そのカテゴリのラベル、およびそのサブカテゴリのラベルを持つ記事を集め、\n",
    "さらに分かち書きしたデータが記録されています（ただし、記事の数や文字数はカテゴリごとに違いがあるので、サイズをそろえるため、それぞれ1MB分のデータを使用しました）。  \n",
    "括弧「()」内のラベルは、jsonファイルにおけるそのカテゴリのキーです。\n",
    "\n",
    "- 動物 (`animal`)\n",
    "- 芸術（`art`)\n",
    "- 経済 (`economy`)\n",
    "- 法 (`law`)\n",
    "- 植物 (`plant`)\n",
    "- 政治 (`politics`)\n",
    "\n",
    "jsonファイル`text/wikipedia_wakati.json`の形式は以下の通りです。   \n",
    "\n",
    "```\n",
    "{\n",
    "    \"[カテゴリキー：例）animal]\": {\n",
    "        \"[記事ID：例）1238]\": {\n",
    "            \"url\": \"[その記事のURL：例）https://ja.wikipedia.org/wiki?curid=1238]\",\n",
    "            \"title\": \"[その記事のタイトル：例）脳科学]\",\n",
    "            \"wakati\": \"[その記事の本文の分かち書き文：例）脳 科学 （ の う か がく 、 ） と は 、...]\"\n",
    "        },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "なお、ウィキペディアのコンテンツは Creative Commons Attribution-ShareAlike 3.0 Unported License (CC-BY-SA) および GNU Free Documentation License (GFDL) の下にライセンスされています。  \n",
    "本授業で配布するデータも同じくこれらのライセンスを継承します。  \n",
    "詳しくは[こちら](https://ja.wikipedia.org/wiki/Wikipedia:%E3%83%87%E3%83%BC%E3%82%BF%E3%83%99%E3%83%BC%E3%82%B9%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89)を参照してください。\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-sa.png\" width=100>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[443, 418, 194, 196, 569, 281]\n",
      "docID: 1238\n",
      "title: 脳科学\n",
      "url: https://ja.wikipedia.org/wiki?curid=1238\n",
      "wakati: 脳 科学 （ の う か がく 、 ） と は 、 ヒト を 含む 動物 の 脳 と 、 それ が 生み出す 機能 について 研究 する 学問 分野 で ある 。 対象 と する 脳 機能 として は 視覚 認知 、 聴覚 認知 など 感覚 入力 の 処理 に関する もの 、 記憶 、 学習 、 予測 、 思考 、 言語 、 問題 解決 など 高次 認知 機能 と 呼ば れる もの 、 情動 に関する もの など で ある 。 \n",
      " 以下 の よう に 様々 な 方法 あるいは 分野 が 存在 し 、 それぞれ 長所 ・ 短所 を 有し て いる 。 2 つ 以上 の 分野 を 同時に 行う こと も ある 。 \n",
      " 例 ： サル に 報酬 課題 を さ せ て いる とき の ドパミン 神経 細胞 の 発火 を 細胞 外 電極 で 測定 する （ ＝ 計算 論 的 神経 科学 ＋ 電気 生理学 ） 。 これ は 有名 な Schultz ら （ 1993 年 ） の 実験 。 \n",
      " 次 の よう に 「 脳 科学 」 という 語 は 学術 分野 において 汎用 さ れ て いる 。 例えば 、 日本 の 公的 な 研究 組織 の 名称 として 、 次 の 組織 に 「 脳 科学 」 の 語 が 使わ れ て いる 。 \n",
      " また 、 専門 書 と 見なせる 書籍 で 「 脳 科学 」 の 用語 が 含ま れ て いる もの として は 、 「 脳 科学 から み た 機能 の 発達 」 、 「 分子 脳 科学 」 、 「 シリーズ 脳 科学 」 、 「 脳 科学 へ の 招待 」 など が ある 。 \n",
      " 理化学研究所   脳 科学 総合 研究 センター   センター 長 の 利根川 進 は 、 当 センター の 研究 対象 として 「 脳 内 の 分子 構造 から 神経 回路 、 認知 ・ 記憶 ・ 学習 の 仕組み 、 健康 と 疾患 等 まで を 研究 対象 と し 、 工学 や 計算 理論 、 心理 学 まで も 含め た 多彩 な 学問 分野 を 背景 に し て 、 学際 的 かつ 融合 的 な 研究 を 目指し て い ます 。 近年 で は 、 分子 や 細胞 といった 微視的 レベル を 扱う 神経 生物 学 と 、 認知 や 計算 論 の よう な 巨視的 レベル と を つなぐ もの として 神経 回路 研究 に 焦点 を 当て て い ます 。 」 と し て いる 。 \n",
      " 一方 、 「 日本 神経 科学 学会 」 の 記述 に よる と 「 日本 神経 科学 学会 は 、 脳 ・ 神経 系 に関する 基礎 、 臨床 及び 応用 研究 を 推進 し 、 その 成果 を 社会 に 還元 、 ひいては 人類 の 福祉 や 文化 の 向上 に 貢献 す べく 、 神経 科学 研究 者 が 結集 し た 学術 団体 です 。 」 と ある 。 神経 科学 の 対象 に は 脳 も 含ま れる し 、 脳 科学 を 研究 する に は 神経 の 研究 も 必要 で ある 。 あえて 分類 すれ ば 、 神経 に は 脳神経 以外 も 含ま れる ため 、 神経 科学 の 方 が より 概念 範囲 が 広い 点 が 違い と 言える 。 \n",
      " 「 脳 科学 者 」 は 日本 の マスメディア に 重宝 さ れ 、 テレビ 番組 に 多数 出演 し 、 数多く の 本 を 執筆 し て いる 。 \n",
      " 「 脳 科学 者 」 の 出版 物 に は 『 脳 内 革命 』 や 『 脳 を 鍛える 大人 の 計算 ドリル 』 の よう に ベストセラー に なっ た もの も 存在 する 。 特に 『 脳 を 鍛える 大人 の 計算 ドリル 』 は 『 脳 を 鍛える 大人 の DS トレーニング 』 として Nintendo   DS で ゲーム 化 さ れ DS 初期 の 人気 ソフト と なっ た 。 \n",
      " 五十音 順 。 脳 科学 に関する メディア 出演 や 執筆 活動 など を通じて 知ら れ た 人物 も 含む 。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "Categories = ['animal', 'art', 'economy', 'law', 'plant', 'politics']\n",
    "\n",
    "with open('text/wikipedia_wakati.json', 'r', encoding='utf-8') as fi:\n",
    "    wiki = json.load(fi)\n",
    "\n",
    "# 各カテゴリに含まれる記事の数\n",
    "print([len(wiki[cate]) for cate in Categories])\n",
    "\n",
    "# 記事を1つだけ出力してみましょう\n",
    "for doc in wiki['animal']:\n",
    "    print('docID:' , doc)\n",
    "    print('title:', wiki['animal'][doc]['title'])\n",
    "    print('url:', wiki['animal'][doc]['url'])\n",
    "    print('wakati:', wiki['animal'][doc]['wakati'])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tf-idfとは\n",
    "\n",
    "### 2.1 tf-idfの定義\n",
    "\n",
    "機械学習のライブラリ「scikit-learn」により、tf-idfという指標を計算することで、各カテゴリにおける重要語を抽出してみましょう。   \n",
    "tf-idfは、文書集合が与えられた時、ある文書において、ある単語はどの程度重要かを表す指標を計算する手法です。   \n",
    "$N$個の文書があったときに、各文書がこれら$N$個の文書集合においてどのような性質があるかを分析するのに用います。  \n",
    "\n",
    "tf-idfは、tf (単語の出現頻度:Term Frequency）とidf（逆文書頻度:Inverse Document Frequency）の二つの指標に基づいて計算されます。   \n",
    "\n",
    "$$\\mbox{tf-idf}(\\mbox{word}_i,\\mbox{doc}_j) = \\mbox{tf}(\\mbox{word}_i,\\mbox{doc}_j) \\cdot \\mbox{idf}(\\mbox{word}_i)\\tag{1}$$\n",
    "\n",
    "$$ \\mbox{tf}(\\mbox{word}_i,\\mbox{doc}_j) = \\frac{\\mbox{doc}_j\\mbox{にword}_i\\mbox{が登場する回数}}{\\mbox{doc}_j\\mbox{の総単語数}}\\tag{2}$$\n",
    "\n",
    "$$ \\mbox{idf}(\\mbox{word}_i) = \\log \\frac{\\mbox{文書の総数}}{\\mbox{word}_i\\mbox{を含む文書の数}}\\tag{3}$$\n",
    "\n",
    "ここで、式(2)は、ある単語$\\mbox{word}_i$がある文書$\\mbox{doc}_j$にたくさん登場していればするほど、   \n",
    "その単語$\\mbox{word}_i$はその文書$\\mbox{doc}_j$において重要であるという指標です。\n",
    "\n",
    "一方、式(3)は、ある単語$\\mbox{word}_i$がいろんな文書に登場するならば、その単語$\\mbox{word}_i$は重要でないとする指標です。   \n",
    "どこにでも現れるような単語（例えば「それ」などはどのカテゴリにもたいてい現れます）は、どの文章においても大して重要でないですが、   \n",
    "特定の文書にしか現れない単語（例えば「脊椎動物」は『動物』のカテゴリくらいしか現れないでしょう）は、その文書（たとえば『動物』）において重要な単語と言えますよね。\n",
    "\n",
    "式(1)はこれら2つの指標を合わせたもので、$\\mbox{word}_i$が文書$\\mbox{doc}_j$においてどれだけ重要かを表す値というわけです。\n",
    "\n",
    "### 2.2  問題設定\n",
    "\n",
    "Wikipediaの6種類のカテゴリ('animal', 'art', 'economy', 'law', 'plant', 'politics')に属する記事を、カテゴリごとに１つの文書にまとめます。   \n",
    "つまり、あるカテゴリに属する複数の記事を一つの文書とみなして分析することで、各カテゴリの重要語を抽出します。   \n",
    "これは、tf-idf値の高い単語を選ぶことにより実現します。\n",
    "\n",
    "tf-idfは自分で計算するプログラムを書いてもそれほど大変ではありませんが、ここでは機械学習用モジュール[scikit-learn](https://scikit-learn.org/stable/index.html)の中の、   \n",
    "`sklearn.feature_extraction.text`というモジュールの`TfidfVectorizer`という関数を使って計算したいと思います。   \n",
    "なお、[マニュアル](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)を読めば、以下では設定していない様々なパラメータを設定することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 入力ベクトルの作成\n",
    "\n",
    "関数`TfidfVectorizer`は、各文書の分かち書き文からなるnumpy形式のリストを入力とします。   \n",
    "今回は6つのカテゴリ＝6つの文書がありますから、6次元のリストということです。  \n",
    "先ほど読み込んだ`wakati`からそのようなリストを作りましょう。   \n",
    "各カテゴリごとにすべての記事の分かち書き文を1つに連結すればいいですね。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "wakati = []\n",
    "for cate in Categories:\n",
    "    text = ''\n",
    "    for item in wiki[cate]:\n",
    "        for line in wiki[cate][item]['wakati']:\n",
    "            text = text + line.replace('\\n','') # 分かち書き文には改行記号が含まているので除去します\n",
    "    wakati.append(text) \n",
    "\n",
    "corpus = np.array(wakati)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どんなリストができたか書き出してみましょう。   \n",
    "ただし、リストの各要素は非常に長い分かち書き文ですので、すべてを書き出すとPCがフリーズする可能性があります。   \n",
    "ここでは、6種類6('animal', 'art', 'economy', 'law', 'plant', 'politics')の各文書に対し、最初の50文字だけ書き出しています。   \n",
    "上から順に、それらしい文の分かち書きが並んでいることが分かりますね。   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in corpus:\n",
    "    print(c[0:50])\n",
    "\n",
    "# それぞれのカテゴリの（空白を含む）文書の長さを出力してみましょう\n",
    "# カテゴリによって記事の数にはばらつきがありますが、文字列の総数は概ね同じです\n",
    "# （各カテゴリで1MB分のデータのみを使用しているためです。実際にはカテゴリによって記事の数も文字数もばらつきがあります。）\n",
    "print([len(v) for v in corpus]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 tf-idfの計算\n",
    "\n",
    "`TfidfVectorizer`を使ってtf-idfを計算してみましょう。  \n",
    "\n",
    "ここで、このような機械学習のプロセスでは、精度を落とさず計算を高速化することを考える必要があります。   \n",
    "あまり情報を持たないデータまでをすべて読み込んで学習しようとすると、データが大きすぎてメモリを圧迫し、\n",
    "コンピュータがフリーズしたり、計算時間がかかる（問題設定によってはいつまでも終わらない）ためです。   \n",
    "（計算時間の話は[「フカシギお姉さん」](https://www.youtube.com/watch?v=Q4gTV4r0zRs)として話題になりましたね。）\n",
    "\n",
    "1. あまり現れない単語は捨てる(`max_features`)  \n",
    "6カテゴリすべての文書集合において、出現頻度の低い単語（つまり全文書集合におけるtf値が低い単語）は\n",
    "あまり重要でないと言えます。出現回数が上位10,000件以下の単語は捨てましょう。これは`max_features`というパラメータで設定できます。   \n",
    "これによって、tf-idf値が計算される単語は10,000種類となり、文書ベクトルの次元も10000次元となります（ただし、文書集合に10,000種類以上の単語が含まれている場合です。10,000単語もない場合は、そこに登場する語彙の数が文書ベクトルの次元となります）。\n",
    "\n",
    "2. あまりたくさんの文書に現れる単語は捨てる(`max_df`)   \n",
    "ここで、idfが何だったかを見返してください。どの文書にも表れるような用語はあまり重要でないと考えるんでしたね。   \n",
    "6カテゴリすべてに現れる単語はidf値が0になるので重要でないですね。   \n",
    "そこで、6カテゴリ中、最大5カテゴリまでに登場する単語のみを考慮しましょう。これは`max_df`というパラメータで設定できます。\n",
    "\n",
    "3. 少ない文書にしか現れない単語は捨てる(`min_df`)   \n",
    "多すぎるのもよくないですが、少なすぎるのもよくありません。tf-idfは、全体の文書集合におけるその文書の性質を計算するためのものですから、ある文書にしか現れない特殊な単語は、他の文書との比較に使えないからです。   \n",
    "そこで、6カテゴリ中、3カテゴリよりも少ないカテゴリにしか現れない単語は除去しましょう。これは`min_df`というパラメータで設定できます。\n",
    "\n",
    "以下でtf-idf値を計算します。   \n",
    "計算結果として得られたXは、6種類の文書と10,000種類の単語の組み合わせに対し、tf-idf値が1つ計算されたものなので、\n",
    "6×10,000次元のベクトルです。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, max_df=5, min_df=3)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.shape) # Xの次元数を出力してみましょう\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 tf-idf値の獲得\n",
    "\n",
    "### 3.1 単語に対するtf-idf値の獲得\n",
    "\n",
    "文書ベクトルとして考慮されることになった10,000種類の単語は文字列としてソートした順番に並んでいます。   \n",
    "何番目にどの単語が登録されているかは以下のようにして確認することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names() # 10,000種類の単語のリストを獲得します\n",
    "\n",
    "# 途中1000番目から1010番目の単語を切り出して表示してみましょう\n",
    "print('単語ID:\\t表記')\n",
    "for i in range(1000, 1010):\n",
    "    print(i, ':\\t', feature_names[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いくつかの単語について、その6種類のカテゴリにおけるtf-idf値を出力してみましょう。   \n",
    "値が高いカテゴリほど、その単語が重要であるといえます。   \n",
    "それぞれの単語で、該当しそうなカテゴリのtf-idf値はほかのカテゴリに比べて高くなっているでしょうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in ['裁判官', '画家', '生育', '細胞', '融資', '衆議院']:\n",
    "    ID = feature_names.index(w)\n",
    "    print('ID: ', ID, '単語：', feature_names[ID])\n",
    "    for cate_n in range(0, len(Categories)):\n",
    "        print('{0:>10}: {1:.4f}'.format(Categories[cate_n], float(X[cate_n,ID])))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 tf-idf値の高い単語リストの獲得\n",
    "\n",
    "各カテゴリではどんな単語が重要（つまりtf-idf値が高い）なのでしょうか？   \n",
    "カテゴリごとに、tf-idfが高い順に単語を並べて表示してみましょう。\n",
    "\n",
    "このためには、カテゴリをキーとして、バリューに単語とその単語のtf-idfの対が、そのtf-idfの降順に並ぶような辞書を作ったらいいですね。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "for cate in Categories:\n",
    "    # 単語とそのtf-idf値の対を辞書として登録\n",
    "    pair = dict(zip(feature_names, X[Categories.index(cate),:].toarray()[0]))\n",
    "    # tf-idfの高い順にソートして、単語とtf-idfの対をタプルとしてリスト化し、辞書に代入する\n",
    "    dic[cate] = [(x, pair[x]) for x in sorted(pair, key=lambda x:-pair[x])]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ではtf-idf値が上から20位までの単語とそのtf-idf値を表示してみましょう。   \n",
    "各カテゴリに特徴的な単語が選ばれていますか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終的に生成した辞書を返却\n",
    "for cate in Categories:\n",
    "    print(cate, dic[cate][:20])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idfによる重要語の可視化\n",
    "\n",
    "せっかくなのでタグクラウドを描画してみましょう。   \n",
    "ここでは、前回使用したwordcloudというモジュールを使います。   \n",
    "WordCloudという関数に対し、前回は分かち書き文をそのまま渡しましたが、  \n",
    "今回は単語をキーとし、そのtf-idfを値とするような辞書を渡します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cate = 'art' # ここを'animal', 'art', 'economy', 'law', 'plant', 'politics'のいずれかに書き換えてみましょう（\n",
    "\n",
    "wordfreq = {w[0]:w[1] for w in dic[cate]}\n",
    "\n",
    "wc = WordCloud(background_color=\"white\",\n",
    "    font_path=\"C:\\Windows\\Fonts\\msgothic.ttc\", # Windows\n",
    "    #font_path=\"/System/Library/Fonts//AppleSDGothicNeo.ttc\", # Mac OS\n",
    "    #font_path='/usr/share/fonts/truetype/fonts-japanese-gothic.ttf', # Colaboratory\n",
    "    width=640,height=480).generate_from_frequencies(wordfreq)\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('fig/TopicAnalysis1-1.png') # 図を画像として保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. tf-idfベクトルによる文書の類似度\n",
    "\n",
    "### 4.1 カテゴリ間の類似度\n",
    "\n",
    "2.4では`TfidfVectorizer`という関数を使って`vectorizer`というモデルを得ました。   \n",
    "`vectorizer = TfidfVectorizer(max_features=10000, max_df=5, min_df=3)`\n",
    "\n",
    "さらにfit_transformというメソッドによって、分かち書きされた入力文`corpus`からtf-idfのベクトルを計算しましたよね。  \n",
    "`X = vectorizer.fit_transform(corpus)`\n",
    "\n",
    "実はこの`vectorizer`はその名の通り、`fit_transform`というメソッドで、分かち書きされた入力文をベクトル化してくれるオブジェクトでした。   \n",
    "それでは、この各カテゴリの文書ベクトル同士の類似度を、コサイン類似度によって評価してみましょう。   \n",
    "結果はBag-of-Wordsのときと同様、類似度行列として出力されます。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(X)\n",
    "print(Categories)\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "値が大きければ大きいほど、その二つのカテゴリの文書ベクトルが類似していることを意味します。   \n",
    "例えばanimalとplanetの類似度は0.38902641で、ほかのカテゴリよりも明らかに高いですね。     \n",
    "また、lawとpoliticsの類似度は0.71316576です。こちらはもっと強い関係があるようです。  \n",
    "類似度行列を詳しく見ると、こんなことが分かるでしょうか？\n",
    "- 「動物」は「植物」と似ている\n",
    "- 「芸術」はどれともあまり似てないけど、あえて言うなら「経済」\n",
    "- 「経済」は「芸術」「法」「政治」と同程度似ている\n",
    "- 「法」と「政治」は似ている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 未知の文書はどのカテゴリの記事かを推定しよう\n",
    "\n",
    "カテゴリが不明なある記事がどのカテゴリの記事なのかを推定する課題に挑戦してみましょう。   \n",
    "`text/wikipedia_sample.json`には、上で読み込んだ`wikipedia.json`と同じ形式ですが、そこには含まれていない記事が記録されています。   \n",
    "6種類のカテゴリについて1つずつ用意しました。   \n",
    "jsonファイルの中を覗けば`Category`も`title`も書いてありますが、もしそれらが分からなかったと仮定して、記事の本文（`text`の要素）だけからどのカテゴリの記事かを推定してみましょう。\n",
    "\n",
    "まず`text/wikipedia_sample.json`を読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text/wikipedia_sample.json', 'r', encoding='utf-8') as fi:\n",
    "    wiki_sample = json.load(fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上ですでに学習済みのvectorizerを使って、未知の文（プログラム中はsampleという変数に代入される）をベクトル化します。  \n",
    "このベクトルの次元は当然10,000次元です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 以下でサンプルのwikipedia記事を読み込みます。このとき、辞書のラベルとして指定されているカテゴリが正解のカテゴリです\n",
    "# 下のうちいずれかの行をコメントアウトして記事を1つ読み込んでください。\n",
    "sample = wiki_sample['animal']['3905242']['wakati'].replace('\\n', '') # 正解は「動物」\n",
    "#sample = wiki_sample['art']['3912545']['wakati'].replace('\\n', '') # 正解は「芸術」\n",
    "#sample = wiki_sample['economy']['204500']['wakati'].replace('\\n', '') # 正解は「経済」\n",
    "#sample = wiki_sample['law']['3000191']['wakati'].replace('\\n', '') # 正解は「法」\n",
    "#sample = wiki_sample['plant']['2043006']['wakati'].replace('\\n', '') # 正解は「植物」\n",
    "# sample = wiki_sample['politics']['725095']['wakati'].replace('\\n', '') # 正解は「政治」\n",
    "sample_tfidf = vectorizer.transform([sample])\n",
    "\n",
    "sample_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この記事について、tf-idf値が高い単語のリストを表示してみましょう。   \n",
    "ただし、ここでリストされるのはsampleに登場するすべての単語ではなく、tf-idfを計算するのに使用したvectorizerを学習する際に考慮した10000個の単語のみです。   \n",
    "sampleの中に、この10000種類に含まれている単語がある程度登場していないと、この記事の特徴量としては非常に粗（スパース）になってしまうので、カテゴリの推定はうまく行かないでしょう。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sample_tfidf[0].nonzero()[1]:\n",
    "    print(feature_names[x], sample_tfidf[0,x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sampleのtf-idfベクトルと、6種類の各カテゴリから計算したtf-idfベクトルとのコサイン類似度を計算し、その結果を棒グラフで描画します。   \n",
    "類似度の高いカテゴリほど、棒グラフが高くなります。   \n",
    "正しいカテゴリの棒グラフが最も高くなったでしょうか？   \n",
    "政治と法はもともと文書ベクトル同士が近いので、この間の判別は難しそうです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sample_sim_matrix = cosine_similarity(sample_tfidf, X)[0]\n",
    "\n",
    "plt.bar([x for x in range(len(sample_sim_matrix))], sample_sim_matrix.flatten(), tick_label=Categories)\n",
    "print(Categories)\n",
    "print(sample_sim_matrix)\n",
    "plt.savefig('fig/TopicAnalysis1-2.png') # 図を画像として保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
