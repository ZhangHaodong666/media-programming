{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 発展的演習（オプション）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colaboratoryで実行する場合\n",
    "以下を実行して、外部ファイルをダウンロードしてください。   \n",
    "**このセルはColaboratoryを起動するたびに必要となります**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "### Colaboratoryのみ以下を実行 ###\n",
    "##################################\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !wget -P ./text http://www.hal.t.u-tokyo.ac.jp/~yamakata/lecture/mediaproc/mediaproc4/wikipedia_base_form_nva.json\n",
    "    !mkdir model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA を用いた教師なし学習によるトピック分類\n",
    "\n",
    "Tf-iDfの課題では、すでにカテゴリごとに分類された記事が大量に用意されていました。 \n",
    "ですから、未分類の文書が与えられたら、分類済みの文書集合を「教師（supervisor)」として、モデルを教師あり学習（supervised learning)することができました。\n",
    "\n",
    "しかし、全く未分類の文書集合がドンと与えられて、「これを適当に分類してね」と言われたらどうしたらいいでしょうか？   \n",
    "あるいは、すでに分類済みであったとしても、「もっと他の分け方ないの？」と言われたら？   \n",
    "（実はこういうことはよくあります。例えば大昔に決めたカテゴリでずっと分類してきたけれど、時代が変わって新しいカテゴリが次々と現れると、大昔のカテゴリ分類は使いづらくなって、改めてゼロからカテゴリを決めたくなるためです。）\n",
    "\n",
    "そんなときは教師なし学習（unsupervised learning)によるトピック分類が有用です。   \n",
    "ここではその一種である潜在的ディリクレ配分法（LDA: Latent Dirichlet Allocation）を使います。   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 前処理\n",
    "\n",
    "人手でラベルが与えられている教師あり学習に比べて、教師なし学習で人間が納得できるような結果を得るためには、事前の処理が極めて重要です。   \n",
    "一般的には次のような処理が行われます。\n",
    "\n",
    "- 単語分割： 日本語では分かち書き、英語ではtokenizationと呼ぶ。先週、janomeを使った分かち書きの方法を学びました。\n",
    "- ストップワード(Stopwords)除去：あまりに一般的過ぎる単語は、頻出する割にはその文書特有の性質を持たないため除去する\n",
    "- ステミング(stemming)：英語において、語幹のみを取り出す処理(例：running --> run-ning, runs --> run-s, runner --> run-ner)。\n",
    "- 見出し語化(lemmatize): ran --> runのように、原形に戻す処理。和文では活用形を原型に戻すなどの処理があります（例：「走っ(た)」-->「走る」）。こうすることにより、集計する際に同じ語彙からなる単語はまとめることができ、文書ベクトルがより密になります。\n",
    "\n",
    "### 1.1 ストップワード除去\n",
    "\n",
    "これがストップワードです！と決められたものはありません。   \n",
    "和文でよく使われているのは、京都大学大学院 情報学研究科 社会情報学専攻 情報図書館学分野（旧・田中研究室）のスタッフや学生が開発してきたWeb情報検索ライブラリ[SlothLib](http://www.dl.kuis.kyoto-u.ac.jp/slothlib/)の中で読み込まれているStopwordsのリストのようです。   \n",
    "その他、処理結果を見て除去したい単語を手作業で追加してもいいでしょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "# SlothLibのStopwordsリストをオンラインから取得\n",
    "res = request.urlopen(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\")\n",
    "stop_words = [line.decode(\"utf-8\").strip() for line in res]\n",
    "\n",
    "# 処理結果を見て手作業でStopwrodsを追加\n",
    "stop_words += ['する','れる','いる','ある','なる','られる','の','せる','おる','行う', '待つ', 'できる','呼ぶ','言う','用いる','持つ','いう']\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 見出し語化・品詞による絞り込み\n",
    "\n",
    "今回は、機能語の中でも特に重要と思われる「名詞」「動詞」「形容詞」の単語のみに限定し、かつ活用形を原型に戻したものを使います。   \n",
    "その結果を`texts/wikipedia_base_form_nva.json`に保存してあるので、それを読み込みます。   \n",
    "これは下のようなものです。\n",
    "\n",
    "元の文章：  \n",
    "`脳 科学 （ の う か がく 、 ） と は 、 ヒト を 含む 動物 の 脳 と 、 それ が 生み出す 機能 について 研究 する 学問 分野 で ある 。 `\n",
    "\n",
    "変換後の文章：  \n",
    "`脳 科学 がく ヒト 含む 動物 脳 生み出す 機能 研究 学問 分野 対象`\n",
    "\n",
    "さらにストップワードを除去します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Wikipediaを読み込み\n",
    "Categories = ['animal', 'art', 'economy', 'law', 'plant', 'politics']\n",
    "\n",
    "with open('text/wikipedia_base_form_nva.json', 'r', encoding='utf-8') as fi:\n",
    "    wiki = json.load(fi)\n",
    "\n",
    "rv_cate = [] # 「正解のカテゴリ分類」を入れるリスト(以下の分類では「わからないもの」とし、使用しない。)\n",
    "base_form_nva = [] # 各記事ごとに、単語を並べたリスト\n",
    "for cate in Categories:\n",
    "    for item in wiki[cate]:\n",
    "        tmp = []\n",
    "        for word in wiki[cate][item]['base_form_nva'].split():\n",
    "            # さらにストップワードを除去します\n",
    "            if word not in stop_words and not word.isdigit():\n",
    "                tmp.append(word)\n",
    "        base_form_nva.append(tmp)\n",
    "        rv_cate.append(cate)\n",
    "        \n",
    "print('全カテゴリの記事数: ', len(base_form_nva))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "記事の総数は2101です。   \n",
    "これらのカテゴリは以下に出力する通りです。  \n",
    "ただし、今回は、『文書が分類されていなかったときの分類』を行うことが目的ですので、これらのカテゴリラベルは学習には使用しません。これは『（機械は知らない）正解のカテゴリ分類結果』と考えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rv_cate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LDAの学習と評価\n",
    "\n",
    "### 2.1 モデル学習\n",
    "トピックモデルを簡単に実装できるgensimというライブラリを使って、LDAモデルを学習してみましょう。   \n",
    "LDAの結果はパラメータによって大きく変わります。   \n",
    "\n",
    "トピック数は、「この文書はいくつのトピックに分けられるか」を考えて決めましょう。   \n",
    "これは6カテゴリから抽出した記事ですから、少なくとも6個のトピックはあると考えられるかもしれません。   \n",
    "また、同じカテゴリでも複数のトピックを含む場合もあるでしょう。   \n",
    "もちろん、複数のカテゴリに現れるトピックもあるでしょう。   \n",
    "ここではとりあえず10個のトピックに分けて見ましょう。\n",
    "\n",
    "また、辞書を作る際、Tf-iDfのときと同じように、すべての単語を考慮すると次元が大きくなりすぎるので、\n",
    "考慮する単語を絞り込みたいと思います。   \n",
    "ここでは、最低出現回数、最大出現頻度、単語数の上限などを指定しています。   \n",
    "\n",
    "このようなパラメータは、これと決まった数字があるわけではありません。   \n",
    "結果を見ながら、手作業でのパラメータチューニングする必要があります。\n",
    "\n",
    "**環境によってはWarningが出るかもしれませんが、とりあえず気にしないでください。**gensimの中で使っているライブラリがアップデートされてバージョンコンフリクトを起こしているためだと思われます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# LDAはランダムパラメータを使うので、実行するたびに結果が変わります\n",
    "# 毎回同じ結果を出したいときは、ランダムシードを設定する必要があります\n",
    "SOME_FIXED_SEED = 6\n",
    "np.random.seed(SOME_FIXED_SEED)\n",
    "\n",
    "# いくつのトピックに分類するかを指定します。この数が増えれば増えるほど、細かく分類されます\n",
    "topic_n =10\n",
    "\n",
    "# 辞書を作成\n",
    "dictionary = Dictionary(base_form_nva)\n",
    "# パラメータを設定\n",
    "dictionary.filter_extremes(no_below=100,# 出現文書数が100回未満の単語を削除\n",
    "                           no_above=0.5,# 出現文書率が50％より大きい単語を削除\n",
    "                           keep_n=1000) # 1000単語以上には増やさない\n",
    "\n",
    "# 各文書をBag-of-Wordsにより文書ベクトルに変換\n",
    "corpus_bow = [dictionary.doc2bow(text) for text in base_form_nva]\n",
    "\n",
    "# LDAの学習\n",
    "lda = LdaModel(corpus=corpus_bow, num_topics=topic_n, id2word=dictionary)\n",
    "\n",
    "# LDAの学習には時間がかかるので、学習したモデルは保存しておきましょう\n",
    "model_pref = 'model/wikipedia_lda'\n",
    "lda.save(model_pref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込みは以下のようにして行います\n",
    "lda = LdaModel.load(model_pref)\n",
    "\n",
    "print(dictionary) # 辞書に登録された単語を見てみましょう\n",
    "print(len(dictionary)) # 辞書に登録されている語彙数を見てみましょう\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 各トピックにおける重要語とその重み\n",
    "\n",
    "ここでは、各トピックごとに、重要度の高かった単語上位5個とその重みを出力しています。   \n",
    "この時点で、トピックごとに見ていったとき、そのトピックがどのようなまとまりであるかが分かるような単語が並んでいるが成功の目安となります。   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tpn in range(topic_n):\n",
    "    print('トピック', tpn, ': ', lda.print_topic(tpn, topn = 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "たとえば以下のようなことが分かるでしょうか。\n",
    "\n",
    "- トピック2は憲法に関する内容\n",
    "- トピック3は経済に関する内容\n",
    "- トピック4と5は政治的な内容\n",
    "- トピック6は芸術に関する内容\n",
    "- トピック8と9は植物に関する内容\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LDAモデルの性能を調べてみよう\n",
    "\n",
    "はたして、このLDAモデルはWikipediaを分類できたでしょうか？\n",
    "\n",
    "LDAは、カテゴリ分類されていない文書集合を分類するものですから、今回のように「動物」「芸術」「経済」というような正解のカテゴリラベルがあるというのはおかしいのですが、今回はこのカテゴリ通りに記事を分類できたかどうかでLDAの分類性能を評価することにします。\n",
    "\n",
    "学習済みのモデル`lda`に対し、各記事の文書ベクトル`corpus_bow`を渡すと、尤度が高かった順に、そのトピック番号と尤度のペアが返されます。   \n",
    "例えば4番目の記事（記事のタイトルは「動物学」、カテゴリは『動物』）のbowを渡すと、第1位のトピックはトピック8で尤度は0.68、第2位のトピックは5で尤度は0.29、第3位のトピックは6で尤度は0.01のようです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(lda[corpus_bow[4]], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分類結果を積み上げグラフで描画してみましょう。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "height = []\n",
    "for cate in Categories:\n",
    "    best = [0] * (topic_n)\n",
    "    for art_n in range(len(base_form_nva)):\n",
    "        if rv_cate[art_n] == cate:\n",
    "            # 尤度が最も高かったトピックを分類結果とする\n",
    "            best[sorted(lda[corpus_bow[art_n]], key=lambda x: x[1], reverse=True)[0][0]] += 1 \n",
    "    height.append(best)\n",
    "    print(cate)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.subplot(1,1,1)\n",
    "btm = np.array([0] * (topic_n))\n",
    "for cate_n in range(len(Categories)):\n",
    "    plt.bar(range(topic_n), height[cate_n], bottom=btm, \n",
    "            tick_label=['Topic {:d}'.format(i) for i in range(topic_n)], \n",
    "            align=\"center\")\n",
    "    btm += np.array(height[cate_n])\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right')\n",
    "plt.legend(Categories, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.subplots_adjust(left = 0.1, right = 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各トピックの重要語から、トピックの性質は以下のようになっていると考えていたはずです。\n",
    "\n",
    "- トピック2は憲法に関する内容\n",
    "- トピック3は経済に関する内容\n",
    "- トピック4と5は政治的な内容\n",
    "- トピック6は芸術に関する内容\n",
    "- トピック8と9は植物に関する内容\n",
    "\n",
    "その通りに分類されているでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今度はWikipediaの分類による各カテゴリの記事が、どの程度の割合で各トピックに分類されたかを可視化してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height2 = np.array(height).T\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "\n",
    "plt.subplot(1,1,1)\n",
    "btm = [0] * len(Categories)\n",
    "for tpn in range(topic_n):\n",
    "    plt.bar(range(len(Categories)), height2[tpn], bottom=btm, tick_label=Categories, align=\"center\")\n",
    "    btm += height2[tpn]\n",
    "\n",
    "plt.legend(['Topic {:d}'.format(i) for i in range(topic_n)], bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.subplots_adjust(left = 0.1, right = 0.75)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "横軸はWikipediaで指定されているカテゴリです。   \n",
    "- 『動物』と『植物』のカテゴリの記事は、概ねトピック８と９に分かれるようです。\n",
    "- トピック6に分類された記事はほとんど『芸術』の記事のようです\n",
    "- トピック3に分類された記事はほとんど『経済』の記事のようです\n",
    "\n",
    "教師なし学習を実用で使う場合は、結果を見ながらパラメータを調整する必要があります。   \n",
    "また、LDAは最初にランダム値をセットするため、計算するたびに答えが変わります（今回はrandom seedを指定しているため変わりません）。   \n",
    "1回うまくいったからと言って、そのパラメータが必ずしも絶対的にいいとは言えないので注意が必要です。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
